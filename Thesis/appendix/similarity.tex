\chapter{String Similarity} \label{app:similarity}
There are different classes of string similarity functions. In this appendix, we focus on the main classes surveyed in the literature and we overview the most popular functions in each class. The similarity formulas described in this appendix compare two strings $s$ and $t$ which are associated with two token sets $S=s_1,s_2,...,s_n$ and $T=t_1,t_2,...,t_m$, respectively. For the computation, we used the Similarity Metric Library available online\footnote{\url{http://sourceforge.net/projects/simmetrics}}.

\section{Token-based Functions} 

The first family of the string similarity is the token-based functions which consider a string as a set of tokens. Intuitively, tokens also called ``bag of words'' are substrings generated by a tokenization function  (e.g. typically by a whitespace) applied on the original string. Making use of token-based functions is advantageous to overcome the word swaps. For example, the similarity between \emph{Mahatma Gandhi} and \emph{Gandhi Mahatma} will be maximal as both strings share the same tokens. However, the main drawback of such functions is to penalize approximate tokens having few spelling variations. That is, the comparison of \emph{brother} and \emph{brothers} will be zero.

One popular function is the Jaccard similarity~\cite{Jaccard:BVSN1901} which is the ratio of the intersection size and the union size of two token sets:

\begin{equation*}
 Jaccard(S,T)=\frac{|S \cap T|}{|S \cup T|}
\end{equation*}

Q-gram is another function which splits a string into small overlapping (i.e. common characters) units of size $q$. To obtain such units with the first and last characters of a string, we introduce a padding character (e.g. \Hash). For example, the 3-grams of \emph{Gandhi} is the set \textit{(\Hash \Hash G,\Hash Ga,Gan,and,ndh,dhi,hi\Hash ,i\Hash \Hash)}. Then, Jaccard function is typically used based on these tokens to compute the similarity score. 

The drawbacks of Jaccard is that it is very sensitive to spelling errors and it significantly penalizes  the unmatched tokens. In contrast, q-gram is less sensitive to spelling errors or to unmatched tokens. This comparison is illustrated in Table~\ref{tab:stringcomp}.

\begin{table}[H]
\footnotesize{
 \begin{center}
  \begin{tabular}{cccc}
  \hline
  String s & String t & Jaccard  & 3-gram \\
  \hline
  Johnny Depp & Johny Dep & 0 & 0.75\\
  \hline 
  sir Johnny Depp & Mr Johnny Depp & 0.5 & 0.78 \\
  \hline 
  \end{tabular}
  \caption{Comparison between Jaccard and 3-gram}
  \label{tab:stringcomp}
 \end{center}}
\end{table}

Cosine distance is another typical token-based function used in Information Retrieval for high dimensional data. Given two n-dimensional vectors X and Y containing the weights of tokens, the Cosine distance is defined as the cosine angle between these two vectors:

\begin{equation*}
 Cosine(X,Y)=\frac{|X \cdot Y|}{\norm{X} \cdot \norm{Y}} = \frac {\sum_{i=1}^{n} x_i \cdot y_i} {\sqrt{\sum_{i=1}^{n} x^{2}_i} \cdot \sqrt{\sum_{i=1}^{n} y^{2}_i}}
\end{equation*}
In particular, the TF-IDF Cosine is commonly used where each token has a weight according to Term Frequency-Inverse Document Frequency scheme. This scheme is composed of two measures: term frequency ($tf$) and  inverse document frequency ($idf$). The intuition behind the term frequency is that the more often a token occurs in a given string, the higher is its contribution to the similarity. In contrast, the inverse document frequency assigns higher weights to rare tokens in all the corpus (all the strings or documents). For each token $s_i$ from the string $s$, the IF-IDF score is:

\begin{equation*}
 tf\textrm{-}idf_{i,s}= tf_{i,s} \cdot log \left(\frac{D}{D(t_i)}\right)
\end{equation*}
where $tf_{i,s}$ is the term frequency of $s_i$ in the string $s$, $D$ is the number of the strings in the corpus, $D(i)$ is the number of strings that contain the token $s_i$ in the corpus. The computation of Cosine distance can be enhanced by hashing functions due to the high sparsity of most vectors. The advantage to use Cosine distance is to take into account the relative importance of different tokens in long strings and text documents.

\section{Character-based Functions}

The second family of string similarity is the character-based function also called edit-based similarity. Unlike the token-based functions, a string is considered as an ordered sequence of characters instead of a set of tokens. They allow different ``edit operations'' necessary to transform one string to another such as deletion, insertion, substitution, and transposition of characters. The use of these functions is mainly performed on short strings to overcome spelling errors. However, the performance of these functions drastically decreases when changing the order of the tokens.

One popular function is the Levenshtein distance~\cite{Levenshtein:SPD66} that allows three edit operations which are the deletion, insertion, and substitution. The score is equal to the minimum number of operations required to transform $s$ to $t$. For example, to transform \textit{Maria} to \textit{Mario}, we need the replace $a$ by $o$, which gives a similarity score equal to 1. The normalized score is equal to 0.8. One drawback of Levenshtein is that it is not adapted for some variations such as abbreviations (e.g. \emph{Gandhi Mahatma} and \emph{Gandhi M}) or extra prefix (e.g. \emph{Sir Gandhi} and \emph{Gandhi}).

A similar metric is the Jaro distance~\cite{Jaro:JASA89} which allows character transpositions and based on the number and the order of common characters. Two characters are considered to be common if they are equal and if the distance between their positions $i$ and $j$ within the two strings does not exceed $H$, where $H=0.5 \times min(|s|,|t|)$. Given a set of common characters $\sigma$, a transposition occurs if the $i^{th}$ common character of $s$ is different from the $i^{th}$ common character of $t$. Let $\theta$ is half the number of transpositions, Jaro is computed as:

\begin{equation*}
		Jaro (s,t) =  \frac{1}{3} \times  \left(  \frac{|\sigma|}{|s|} +\frac{|\sigma|}{|t|} + \frac{|\sigma|-\theta}{|\sigma|} \right) 
\end{equation*}
Jaro distance performs well when there is few spelling variations. However, as common characters have to occur in a specific distance, variations such as a long prefix in one string (e.g. $s=\textit{Doctor John Smith}$ and $t=\textit{John Smith}$) yields a low similarity of 0.46. A variant of Jaro distance, called Jaro-Winkler similarity~\cite{Winkler:1999} uses the length of the longest common prefix to emphasize matches in the first $p$ characters of the two strings. For example the Jaro similarity between \textit{John S} and \textit{John Smith} is 0.86, while the Jaro-Winkler score is 0.94.

\section{Hybrid Functions} 
To overcome the limitations of character and token based functions, the metrics in the third family combines both of them, also referred as hybrid functions.

Extended Jaccard similarity is a hybrid function proposed to also include not only the equals tokens, but also the similar ones in the the original Jaccard function~\cite{Weis:SIGMOD05,Ananthakrishna:VLDB02}. Consider \textit{TokenSim} be a string similarity metric that compares two tokens $s_i$ and $t_j$, and $\theta$ is the related threshold, the set of shared similar tokens between $s$ and $t$ is defined as:

\begin{equation*}
	Shared (s,t)=\lbrace (s_i,t_j) | s_i \in S \wedge t_j \in T: TokenSim(s_i,t_j)\geqslant \theta \rbrace
\end{equation*}
The set of unique or unmatched tokens in $s$ is defined as:

\begin{equation*}
	Unique(s)=\lbrace s_i | s_i \in S \wedge t_j \in T \wedge (s_i,t_j) \notin Shared \rbrace
\end{equation*}
Similarly, we define the set $Unique(t)$ for the string $t$. This has been extended by a function that gives weights $w$ to matched and unmatched tokens, which are combined using an aggregation function $Ag$. The hybrid Jaccard is defined as:
\vspace{-2mm}
\begin{gather*}
	matched=Ag_{(s_i,t_j) \in Shared (s,t)} {\ } w(s_i,t_j) \\[1mm]
	unmatched=Ag_{(s_i) \in Unique(s)}  {\ } w(s_i)  + Ag_{(t_j) \in Unique(t)} {\ }  w(t_j)  \\[1mm]
	HybridJaccard(s,t)= \frac{matched}{matched+unmatched}
\end{gather*}
Note that different weights could be given for the tokens in $Shared (s,t)$, $Unique(s)$, and $Unique(t$). For instance, let s= \textit{Mindy Smith} and t=\textit{Minndy Smith Festival}, the hybrid Jaccard generates the following sets: 
\begin{gather*}
Shared(s,t)= \lbrace (Mindy,Minndy),(Smith,Smith) \rbrace \\[1mm]
 Unique(s)= \varnothing  \\[1mm]
 Unique(t)=  \lbrace Festival \rbrace
\end{gather*}
Assuming that the weights of matched tokens is their normalized Levenshtein similarity, and the weights of unmatched tokens is equal to 1. If the aggregate function $Ag$ simply sums the weights, the hybrid Jaccard is:  

\begin{equation*}
	HybridJaccard(s,t)=\frac{0.83+1}{0.83+1+0+1}=0.64
\end{equation*}
Note that the score remains low due to the influence of unmatched tokens. The Token-Wise metric proposed in Section~\ref{sec:metric-similarity} follows the same rationale, but gives more importance to similar tokens. Moreover, the weight of unmatched tokens takes into account the fact that the two token sets have different sizes. In this example, the weight for unmatched tokens is equal to $\frac{2}{3}=0.66$. We obtain higher score than hybrid Jaccard when using Token-wise:

\begin{equation*}
	Token\textrm{-}Wise(s,t)=\frac{2 \times (0.83+1)}{2 \times (0.83+1)+ 0.66 \times (0+1)}=0.84
\end{equation*}
Another hybrid function is the Monge-Elkan similarity~\cite{Monge:KDD96} that matches every token $s_i$ from $s$ with the token $t_j$ in $t$ having the maximum similarity using \textit{TokenSim} metric. Monge-Elkan is defined as:

\begin {equation*}
MongeElkan(s,t) = \frac{1}{|S|} {\ } \sum_{i=1}^{|S|}  \max_{j=1}^{|T|} {\ } TokenSim(s_i,t_j)
\end {equation*}
Given the strings $s=$ \textit{Mindy Smith} and $t=$ \textit{Minndy Smith Festival}, and using Levenshtein as \textit{TokenSim}, the Monge-Elkan score is:

\begin{equation*}
	MongeElkan(s,t)=\frac{0.83+1}{2}=0.91
\end{equation*}
Monge-Elkan is sensitive to the size of the first string. For instance, if $t$ is the first string which is of length 3, the Monge-Elkan score decreases to 0.61.
\\
\\
The last hybrid function is called SoftTFIDF~\cite{Cohen:IIWeb03} which extends the Cosine similarity, following the same rationale as hybrid Jaccard. Let $CLOSE(\theta, S, T)$ be the set of words $s_i \in S$ such that there is $t_j \in T$ where $TokenSim(s_i,t_j) > \theta$, and $maxsim(s_i,t_j)=max(\lbrace TokenSim(s_i,t_j) | t_j\in T \rbrace)$. The SoftTFIDF is defined as:

\begin {equation*}
SoftTFIDF(s,t) = \sum_{s_i \in CLOSE(\theta, S, T)} \left( \frac{tf\textrm{-}idf_{s_i}}{\norm{X}} \cdot \frac{tf\textrm{-}idf_{t_j}}{\norm{Y}} \times maxsim(s_i,t_j) \right) 
\end {equation*}
where $X$ and $Y$ are the vector representations of $s$ and $t$ containing the $tf\textrm{-}idf$ scores of related tokens, respectively. Given the strings $s=$ \textit{Mindy Smith} and $t=$\textit{Minndy Smith Festival} and unit weights for all the tokens (no corpus considered), the SoftTFIDF gives:

\begin{equation*}
	SoftTFIDF(s,t)=\frac{1}{\sqrt{2}} \times \frac{1}{\sqrt{3}} \times 0.83 + \frac{1}{\sqrt{2}} \times \frac{1}{\sqrt{3}} \times 1 =0.75
\end{equation*}
