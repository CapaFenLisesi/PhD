%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Âµ%%%%%%%%%%%%%%%%%%
%%%  The State of Linked Data - An Extensible Framework to Asses and Build Dataset Profiles            %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{url}
\usepackage{listings}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{algorithm}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

% todo macro
\usepackage{color}
\newtheorem{deflda}{Axiom}
\newcommand{\todo}[1]{\noindent\textcolor{red}{{\bf \{TODO}: #1{\bf \}}}}

% Language Definitions for Turtle
\definecolor{olivegreen}{rgb}{0.2,0.8,0.5}
\definecolor{grey}{rgb}{0.5,0.5,0.5}
\lstdefinelanguage{ttl}{
sensitive=true,
morecomment=[l][\color{brown}]{@},
morecomment=[l][\color{red}]{\#},
morestring=[b][\color{blue}]\",
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Beginning of document  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% first the title is needed
\title{The State of Linked Data}
\subtitle{An Extensible Framework to Asses and Build Dataset Profiles}

\author{Ahmad Assaf\inst{1}\inst{2}, Aline Senart\inst{2} and Rapha\"{e}l Troncy\inst{1} }

\institute{EURECOM, Sophia Antipolis, France. \email{<firstName.lastName@eurecom.fr>}
  \and SAP Labs France. \email{<firstName.lastName@sap.com>}
}

% a short form should be given in case it is too long for the running head
\titlerunning{The State of Linked Data - An Extensible Framework to Asses and Build Dataset Profiles}
%\authorrunning{Assaf, Senart and Troncy}

\maketitle

%%%%%%%%%%%%%%%%%%
%%%  Abstract  %%%
%%%%%%%%%%%%%%%%%%

\begin{abstract}
Linked Open Data (LOD) has emerged as one of the largest collection of interlinked datasets on the web. Benefiting from this mine of data requires the existence of descriptive information about each dataset in the accompanying metadata. Such meta information is currently very limited to few Data Portals where they are provided manually thus giving little or bad quality insights. To address this issue, we propose a scalable automatic approach for extracting and generating descriptive linked dataset meta information. This approach apply several techniques to check the validity of the attached metadata of a certain dataset as well as a whole Data Portal. Using our framework on prominent Data Portals shows that the general state of the Linked Open Data needs attention as most of datasets suffer from ad quality metadata and lack additional informative metrics.
\keywords{Linked Data, Dataset Profile, Metadata, Data Quality}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  1. Introduction  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}
In the last few years the Semantic Web gained a momentum supported by the introduction of many related initiatives like the Linked Open Data (LOD)\cite{BizerHeath2009}. From 12 datasets cataloged in 2007, the Linked Open Data has grown to almost 1000 datasets containing almost 82 billion triples\footnote{http://datahub.io/dataset?tags=lod}. Data is being published by both public and private sectors and covers a diverse set of domains from life sciences to military.

The Linked Open Data is a gold mine for organizations and individuals who are trying to leverage external data sources in order to produce more informed business decisions \cite{Boyd2011}. This success lies in the cooperation between data publishers and consumers. Users are empowered to find, share and combine information in their applications easily. However, the heterogeneous nature of data sources reflects directly on the data quality as these sources often contain inconsistent as well as misinterpreted and incomplete information and meta information. Accompanied with the significant variation of size, used languages and freshness, finding useful datasets without prior knowledge is increasingly complicated. This can be clearly noticed in the LOD Cloud \footnote{http://lod-cloud.net} as few datasets like DBPedia\cite{Bizer:2009:DCP:1640541.1640848}, Freebase\cite{Bollacker:2008:FCC:1376616.1376746} and YAGO\cite{Suchanek:2007:YCS:1242572.1242667} are favored over hidden gems that may include domain specific knowledge more suitable for the tasks on hand.

The main entry point for discovering and identifying needed datasets is through public Data Portals like DataHub\footnote{http://datahub.io} and Europe's Public Data\footnote{http://publicdata.eu} or private ones like Quandl\footnote{https://quandl.com/} and Engima\footnote{http://enigma.io/}. Private portals harness manually curated data from various sources and expose them to users either freely or through paid plans. The data available is of higher quality but lesser quantity compared to what is available in public portals. Similarly in some public Data Portals, administrators manually review datasets information and attach suitable meta information. This information is mainly in form of predefined tags such as \textit{media, geography, life sciences, etc.} for organization and clustering purposes.
The increasing number of available datasets makes the review and curation process unsustainable even when outsourced to communities. Furthermore, the diversity of those datasets makes it hard to classify them with a fixed number of predefined tags that can be subjectively assigned without capturing the essence and breadth of the dataset\cite{6690016}.

\textit{Data profiling} is the process of creating descriptive dataset metadata. It is a cardinal activity when facing an unfamiliar dataset\cite{semwebprofiling}.It helps in assessing the importance of the dataset, improve users' ability to search and reuse part of the dataset and detect irregularities to improve its quality. Data profiling includes several tasks:
\begin{itemize}
  \item \textbf{Metadata profiling}: Provide several information about the dataset. This can include general information (dataset description, release and update dates, etc.), legal information (license information, openness, etc.), practical information (access points, data dumps, etc.) and so on.
  \item \textbf{Statistical profiling}: Provides statistical information about data types and patterns in the dataset. i.e. properties distribution, number of entities and RDF triples, etc.
  \item \textbf{Topical profiling}: Provides descriptive and reliable knowledge on the dataset's content and structure. This can be in form of tags and categories used to facilitate search and reuse of existing datasets.
\end{itemize}

In this work, we address the above mentioned challenges of automatic assessment and generation of descriptive datasets profiles. This paper proposes an extensible framework consisting of a processing pipeline that combines techniques for Data Portals identification, datasets crawling and a set of pluggable modules combining several profiling tasks. The framework assesses the provided dataset metadata against an aggregated standard set of information. Metadata fields are automatically corrected when possible i.e. missing license link. Moreover, a report is created with issues that cannot be automatically fixed and is sent to the dataset's maintainer via e-mail. There exist various statistical and topical profiling tools for both relational and Linked Data. The architecture of the framework allows to easily add several profiling tasks. For this paper, we will focus on Linked Data profiling tools as we will present our findings on the overall state of Linked Data in some of the prominent Data Portals.

The remainder of the paper is structured as follows. Section 2 reviews related literature. Section 3 describes the framework's architecture to assess and generate dataset profiles. Section 4 shows the results of running this tool on some of the most prominent Data Portals and their discussion. Finally, Section 5 presents the conclusion and future work.


\bibliographystyle{abbrv}
\nocite{*}
\bibliography{SOLD}
\end{document}
